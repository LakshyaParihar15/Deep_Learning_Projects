{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ff75a49-a49f-4cc0-9070-537efc277564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a82743-a99d-49ca-a858-ad99d17a6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in training, validation, and test datasets from text files\n",
    "train_data = pd.read_csv('train.txt', sep = ';', header=None, names=[\"text\", \"emotions\"])\n",
    "val_data = pd.read_csv('val.txt', sep = ';', header=None, names=[\"text\", \"emotions\"])\n",
    "test_data = pd.read_csv('test.txt', sep = ';', header=None, names=[\"text\", \"emotions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a5527a-2c43-41de-98eb-1f228c4f56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding emotion labels into numerical values using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_data['encoded_label'] = label_encoder.fit_transform(train_data['emotions'])\n",
    "val_data['encoded_label'] = label_encoder.fit_transform(val_data['emotions'])\n",
    "test_data['encoded_label'] = label_encoder.fit_transform(test_data['emotions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59cffd87-1951-4326-a01c-f6c05ce49972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sentences (text) and corresponding encoded emotion labels\n",
    "train_sentences = train_data['text'].tolist()\n",
    "train_labels = train_data['encoded_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194b488f-6276-4276-a6c0-018775704528",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sentences = val_data['text'].tolist()\n",
    "val_labels = val_data['encoded_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1ca8d1-34f1-4f5d-a8ff-35343ba33589",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test_data['text'].tolist()\n",
    "test_labels = test_data['encoded_label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72075e1e-271a-4aa2-8c51-b29def3aabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62e488c-7997-475b-8073-cc144cdc733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing, lemmatizing, and removing stop words from the sentences\n",
    "train_tokens = []\n",
    "for sentence in train_sentences:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    train_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3507b2d-631d-4665-bde2-03ac4d9cc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tokens = []\n",
    "for sentence in val_sentences:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    val_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9552e7f6-61ea-410d-9a04-7c141537770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = []\n",
    "for sentence in test_sentences:\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    test_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d789a0f-686b-49ac-8c86-0b0917598245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras Tokenizer for converting words to sequences and padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d3140c4-67cf-46c3-b988-095fe19d468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining parameters for the tokenizer and padding\n",
    "max_vocab_size = 10000  # Maximum number of words to keep in vocabulary\n",
    "max_sequence_length = 100  # Maximum length of the sequences after padding\n",
    "\n",
    "# Tokenizing the train data and padding the sequences\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(train_tokens)  # Fitting tokenizer to the training tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "092e9c64-4f29-4c8e-948a-03534da0ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_tokens), maxlen=max_sequence_length, padding='post')\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_tokens), maxlen=max_sequence_length, padding='post')\n",
    "X_test = pad_sequences(tokenizer.texts_to_sequences(test_tokens), maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66883935-fc46-4116-8de7-c0eff25daa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting labels to numpy arrays\n",
    "import numpy as np\n",
    "y_train = np.array(train_data['encoded_label'])\n",
    "y_val = np.array(val_data['encoded_label'])\n",
    "y_test = np.array(test_data['encoded_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "185ef68e-b52c-473a-b444-99991289f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary Keras modules for building the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
    "\n",
    "# Number of classes based on the label encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Defining the model architecture with a Bidirectional LSTM layer\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128),\n",
    "    Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ac03c86-75c9-43b0-9655-ae88ae06b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model with Adam optimizer and sparse categorical cross-entropy loss\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a097299-0e3f-44ef-9e11-0c81e374b086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 74ms/step - accuracy: 0.4792 - loss: 1.3365 - val_accuracy: 0.8770 - val_loss: 0.3445\n",
      "Epoch 2/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 75ms/step - accuracy: 0.9269 - loss: 0.2098 - val_accuracy: 0.9100 - val_loss: 0.2378\n",
      "Epoch 3/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 75ms/step - accuracy: 0.9621 - loss: 0.1050 - val_accuracy: 0.9045 - val_loss: 0.2505\n",
      "Epoch 4/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 76ms/step - accuracy: 0.9747 - loss: 0.0701 - val_accuracy: 0.9140 - val_loss: 0.2448\n",
      "Epoch 5/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 77ms/step - accuracy: 0.9820 - loss: 0.0509 - val_accuracy: 0.9095 - val_loss: 0.3063\n",
      "Epoch 6/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 81ms/step - accuracy: 0.9862 - loss: 0.0405 - val_accuracy: 0.9120 - val_loss: 0.2864\n",
      "Epoch 7/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 332ms/step - accuracy: 0.9901 - loss: 0.0314 - val_accuracy: 0.9150 - val_loss: 0.2943\n",
      "Epoch 8/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.9875 - loss: 0.0320 - val_accuracy: 0.9090 - val_loss: 0.3282\n",
      "Epoch 9/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.9924 - loss: 0.0228 - val_accuracy: 0.9105 - val_loss: 0.3641\n",
      "Epoch 10/10\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 77ms/step - accuracy: 0.9902 - loss: 0.0235 - val_accuracy: 0.9060 - val_loss: 0.3541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22ec3a819d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model on the training data and validating it on the validation data\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17fe421f-0347-477a-9d79-cc0dd618be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9001 - loss: 0.3779\n",
      "Test Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf5e96b-9b45-4078-b8bb-1368f780b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# Decoding the predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = [label_encoder.classes_[np.argmax(pred)] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc86b29-c2de-4372-a387-0b92b9028b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.91      0.87      0.89       275\n",
      "        fear       0.81      0.92      0.86       224\n",
      "         joy       0.92      0.95      0.93       695\n",
      "        love       0.84      0.68      0.75       159\n",
      "     sadness       0.94      0.94      0.94       581\n",
      "    surprise       0.88      0.67      0.76        66\n",
      "\n",
      "    accuracy                           0.90      2000\n",
      "   macro avg       0.88      0.84      0.86      2000\n",
      "weighted avg       0.90      0.90      0.90      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating a classification report to evaluate the model's performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_data['emotions'], predicted_labels, target_names=label_encoder.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
